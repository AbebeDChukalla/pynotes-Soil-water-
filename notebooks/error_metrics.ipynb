{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Metrics\n",
    "\n",
    "Qunatifying the error between predictions and observations is central to assessing the performance of a model. Error metrics also play an important role assessing the accuracy of sensors relative to ground-truth observations. \n",
    "\n",
    "There are many error metrics, each with its pros and cons. In this exercise we will code some of the most common error metrics and we will briefly discuss their application. There are also several Python libraries (e.g. sciki-learn) that contain multiple error metrics. Using libraries is a good idea since it allows for reproducibility and there is also a high chance that the code has been tested by multiple people. However, the compulsive use of libraries by beginners also hinders understanding. This is the reason why in this exercise we will code our own error metrics.\n",
    "\n",
    ">In most examples below the error metrics are self-contained. This means that in most cases the computation of an error metric does not rely on the results of another cell. This is certainly not ideal, but from the point of view of learning the computation of the different error metrics it prevents tracking back the different variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "We will use a dataset obtained from the calibration of an actual soil moisture sensor. The soil moisture readings were obtained with a calibrated soil water reflectometer and the ground-truth values we re obtained using the thermo-gravimetric method (i.e. soil samples were oven-dried to find out the actual soil water content).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "y_obs =  [0.190,0.438,0.304,0.408,0.003,0.459,0.409,0.403,0.174,0.033,0.317,0.023]\n",
    "y_pred = [0.233,0.481,0.319,0.450,0.004,0.466,0.458,0.497,0.166,0.013,0.396,0.003]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Numpy array\n",
    "y_obs = np.array(y_obs)\n",
    "y_pred = np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals\n",
    "\n",
    "The difference between observed and predicted: $residuals = y_{obs} - y_{pred}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.043 -0.043 -0.015 -0.042 -0.001 -0.007 -0.049 -0.094  0.008  0.02\n",
      " -0.079  0.02 ]\n"
     ]
    }
   ],
   "source": [
    "residuals = y_obs - y_pred\n",
    "print(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of residuals (SRES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.325\n"
     ]
    }
   ],
   "source": [
    "sres = np.nansum(residuals)\n",
    "print(sres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of the absolute of residuals (SARES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4210000000000001\n"
     ]
    }
   ],
   "source": [
    "sares = np.nansum(np.abs(residuals))\n",
    "print(sares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of squared errors (or residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024079000000000007\n"
     ]
    }
   ],
   "source": [
    "sse = np.nansum(residuals**2)\n",
    "print(sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean bias error (MBE)\n",
    "\n",
    "**Positive** = under-prediction. \n",
    "\n",
    "**Negative** = Over-prediction.\n",
    "\n",
    "A bias equal to zero can be a consequence of small errors or very large errors balanced by opposite sign. It is always recommended to include other error metrics in addition to the mean bias error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.027083333333333334\n"
     ]
    }
   ],
   "source": [
    "mbe = np.nanmean(residuals)\n",
    "print(mbe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0020065833333333337\n"
     ]
    }
   ],
   "source": [
    "mse = np.nanmean(residuals**2)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root mean squared error (RMSE)\n",
    "\n",
    "One of the most popular error metrics in modeling. When comparing two estimates where none of them represents the ground truth it is better to name this error metric the Root Mean Squared \"Difference\" to emphasize that is the difference between two estimates. the word error is typically reserved for deviations from a gold standard or ground truth value.\n",
    "\n",
    ">Sensitive to outliers. The computation of the mean of the squared residuals is largely dominated by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04479490298385893\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(np.nanmean(residuals**2))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative root mean squared error (RRMSE)\n",
    "\n",
    "More meaningful than RMSE to compare the errors from two different data sets with different units or ranges. Sometimes the RRMSE is computed by dividing the RMSE over the range of the observed values rather than the average of the observed values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1700534121500497\n"
     ]
    }
   ],
   "source": [
    "rrmse = np.sqrt(np.nanmean(residuals**2)) / np.nanmean(y_obs)\n",
    "print(rrmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean absolute error (MAE)\n",
    "\n",
    "Robust error metric less sensitive to outliers compared to the RMSE. The computation of the mean of the absolute residuals is not squared, and thus less impacted by outliers. Read Willmott and Matsuura, 200 for more details about RMSE vs MAE.\n",
    "\n",
    "For even a more robust error metric you can compute the median absolute error, which represents the most typical error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03508333333333334\n"
     ]
    }
   ],
   "source": [
    "mae = np.nanmean(np.abs(residuals))\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Willmott index of agreement (D)\n",
    "\n",
    "A perfect model will have y_obs = y_pred, and consequently the SSE will be zero, and hence d=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9841462973021908\n"
     ]
    }
   ],
   "source": [
    "abs_diff_pred = np.abs(y_pred - np.nanmean(y_obs))\n",
    "abs_diff_obs  = np.abs(y_obs  - np.nanmean(y_obs))\n",
    "\n",
    "willmott = 1 - np.nansum(residuals**2) / np.nansum((abs_diff_pred + abs_diff_obs)**2)\n",
    "print(willmott)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Willmott, C.J., Robeson, S.M. and Matsuura, K., 2012. A refined index of model performance. International Journal of Climatology, 32(13), pp.2088-2094.\n",
    "\n",
    "Willmott, C.J. and Matsuura, K., 2005. Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance. Climate research, 30(1), pp.79-82.\n",
    "\n",
    "Willmott, C.J., 1981. On the validation of models. Physical geography, 2(2), pp.184-194."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
